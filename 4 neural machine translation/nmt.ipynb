{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "褚则伟 zeweichu@gmail.com\n",
    "\n",
    "本段代码作为给稀牛学院学员参考之用。\n",
    "本代码实现了Sequence to Sequence模型的一个参考，如有任何bug建议学员自行修复，也欢迎email汇报给我。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n",
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "train_file = \"./data/train.txt\"\n",
    "enc = sys.getdefaultencoding()\n",
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding = enc) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            # comment the following line if you use nltk\n",
    "            en.append([\"BOS\"] + line[0].split() + [\"EOS\"])\n",
    "            # uncomment the following line if you installed nltk\n",
    "#             en.append([\"BOS\"] + nltk.word_tokenize(line[0]) + [\"EOS\"]) \n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "train_en, train_cn = load_data(train_file)\n",
    "num_train = len(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', 'Anyone', 'can', 'do', 'that.', 'EOS'],\n",
       " ['BOS', 'How', 'about', 'another', 'piece', 'of', 'cake?', 'EOS'],\n",
       " ['BOS', 'She', 'married', 'him.', 'EOS'],\n",
       " ['BOS', 'I', \"don't\", 'like', 'learning', 'irregular', 'verbs.', 'EOS'],\n",
       " ['BOS', \"It's\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me.', 'EOS']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS'],\n",
       " ['BOS', '要', '不', '要', '再', '來', '一', '塊', '蛋', '糕', '？', 'EOS'],\n",
       " ['BOS', '她', '嫁', '给', '了', '他', '。', 'EOS'],\n",
       " ['BOS', '我', '不', '喜', '欢', '学', '习', '不', '规', '则', '动', '词', '。', 'EOS'],\n",
       " ['BOS',\n",
       "  '這',\n",
       "  '對',\n",
       "  '我',\n",
       "  '來',\n",
       "  '說',\n",
       "  '是',\n",
       "  '個',\n",
       "  '全',\n",
       "  '新',\n",
       "  '的',\n",
       "  '球',\n",
       "  '類',\n",
       "  '遊',\n",
       "  '戲',\n",
       "  '。',\n",
       "  'EOS']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "model_dir = \"seq2seq\"\n",
    "make_dir(model_dir)\n",
    "\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 1\n",
    "    word_dict = {w[0]: index+1 for (index, w) in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = 0\n",
    "    return word_dict, total_words\n",
    "\n",
    "vocab_file = os.path.join(model_dir, \"vocab.pkl\")\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "    \n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将word转换成index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = []\n",
    "    out_cn_sentences = []\n",
    "\n",
    "    for i in range(length):\n",
    "        en_seq = [en_dict[w] if w in en_dict else 0 for w in en_sentences[i]]\n",
    "        cn_seq = [cn_dict[w] if w in cn_dict else 0 for w in cn_sentences[i]]\n",
    "        out_en_sentences.append(en_seq)\n",
    "        out_cn_sentences.append(cn_seq)\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 9060, 2], [1, 4291, 2], [1, 2733, 2], [1, 5635, 2], [1, 4291, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 7, 87, 444, 5, 3, 1],\n",
       " [2, 118, 1399, 220, 1],\n",
       " [2, 1000, 2146, 7, 3, 1],\n",
       " [2, 237, 237, 220, 1],\n",
       " [2, 153, 188, 220, 1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据转换成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "import numpy as np\n",
    "\n",
    "# get minibatches of \n",
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, max_len)).astype('float32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        x_mask[idx, :lengths[idx]] = 1.0\n",
    "    return x, x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_mask = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_mask = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_mask, mb_y, mb_y_mask))\n",
    "    return all_ex\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tf.contrib.seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, embedding, hidden_size, num_layers = 1):\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell = rnn.GRUCell(self.hidden_size)\n",
    "        \n",
    "    def __call__(self, inputs, seq_length, state=None):\n",
    "        out = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        for i in range(self.num_layers):\n",
    "            out, state = tf.nn.dynamic_rnn(self.cell, out, sequence_length=seq_length, initial_state=state, dtype=tf.float32)\n",
    "        return out, state\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, embedding, hidden_size, num_layers=1, max_length=15):\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell = rnn.GRUCell(hidden_size)\n",
    "        self.linear = tf.Variable(tf.random_normal(shape=(self.hidden_size, cn_total_words))*0.1)\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs, state, encoder_state): # context vector\n",
    "        \n",
    "        out = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        out = tf.tile(tf.expand_dims(encoder_state, 1), (1, tf.shape(out)[1], 1))\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "#             state = tf.concat([state, encoder_state], 1)\n",
    "            out, state = tf.nn.dynamic_rnn(self.cell, out, initial_state=state, dtype=tf.float32)\n",
    "    \n",
    "        out = tf.tensordot(out, self.linear, axes=[[2], [0]])\n",
    "        return out, state\n",
    "\n",
    "class Seq2Seq:\n",
    "    def __init__(self, hidden_size, num_layers, embed_words_en, embed_words_cn):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = 15\n",
    "        self.grad_clip = 5.0\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            with tf.name_scope(\"place_holder\"):\n",
    "                self.encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"encoder_inputs\")\n",
    "                self.encoder_length = tf.placeholder(shape=(None, ), dtype=tf.int64, name=\"encoder_length\")\n",
    "                self.decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"decoder_inputs\")\n",
    "                self.decoder_target = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"decoder_target\")\n",
    "                self.decoder_mask = tf.placeholder(shape=(None, None), dtype=tf.float32, name=\"decoder_mask\")\n",
    "\n",
    "            with tf.name_scope(\"embedding\"):\n",
    "                self.embedding_en = tf.get_variable(name=\"embedding_en\", dtype=tf.float32, shape=(en_total_words, hidden_size),\n",
    "                                                    initializer=tf.constant_initializer(embed_words_en))\n",
    "                self.embedding_cn = tf.get_variable(name=\"embedding_cn\", dtype=tf.float32, shape=(cn_total_words, hidden_size),\n",
    "                                                    initializer=tf.constant_initializer(embed_words_cn))\n",
    "            with tf.name_scope(\"encoder-decoder\"):\n",
    "                self.encoder = Encoder(self.embedding_en, self.hidden_size, self.num_layers)\n",
    "                self.decoder = Decoder(self.embedding_cn + self.hidden_size, self.hidden_size, self.num_layers)\n",
    "\n",
    "            with tf.variable_scope(\"seq2seq-train\"):\n",
    "                encoder_outputs, encoder_state = self.encoder(self.encoder_inputs, self.encoder_length)\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                decoder_state = encoder_state\n",
    "                word_indices = self.decoder_inputs\n",
    "\n",
    "                decoder_outputs, decoder_state = self.decoder(word_indices, decoder_state, encoder_state)\n",
    "\n",
    "                # decoder_outputs.append(decoder_out)\n",
    "                decoder_outputs = tf.concat(decoder_outputs, 1)\n",
    "\n",
    "            with tf.name_scope(\"cost\"):\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_outputs, labels=self.decoder_target)\n",
    "\n",
    "                self.cost = tf.reduce_mean(loss * self.decoder_mask)\n",
    "\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), self.grad_clip)\n",
    "                optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "            with tf.variable_scope(\"seq2seq-generate\"):\n",
    "                self.generate_outputs = []\n",
    "                decoder_state = encoder_state\n",
    "                word_indices = tf.expand_dims(self.decoder_inputs[:, 0], 1)\n",
    "                for i in range(self.max_length):\n",
    "                    decoder_out, decoder_state = self.decoder(word_indices, decoder_state, encoder_state)\n",
    "                    softmax_out = tf.nn.softmax(decoder_out[:, 0, :])\n",
    "                    word_indices = tf.expand_dims(tf.cast(tf.argmax(softmax_out, -1), dtype=tf.int64), 1)\n",
    "                    self.generate_outputs.append(word_indices)\n",
    "                self.generate_outputs = tf.concat(self.generate_outputs, 0)\n",
    "            \n",
    "            \n",
    "    def train(self, sess, encoder_inputs, encoder_length, decoder_inputs, decoder_target, decoder_mask):\n",
    "        _, cost = sess.run([self.train_op, self.cost], feed_dict={\n",
    "            self.encoder_inputs: encoder_inputs, \n",
    "            self.encoder_length: encoder_length,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target: decoder_target,\n",
    "            self.decoder_mask: decoder_mask\n",
    "        })\n",
    "        return cost\n",
    "    \n",
    "    def generate(self, sess, encoder_inputs, encoder_length):\n",
    "        decoder_inputs = np.asarray([[en_dict[\"BOS\"]]*15], dtype=\"int64\")\n",
    "        if encoder_inputs.ndim == 1:\n",
    "            encoder_inputs = encoder_inputs.reshape((1, -1))\n",
    "            encoder_length = encoder_length.reshape((-1))\n",
    "        generate = sess.run([self.generate_outputs],\n",
    "                           feed_dict={self.encoder_inputs: encoder_inputs,\n",
    "                                      self.decoder_inputs: decoder_inputs,\n",
    "                                      self.encoder_length: encoder_length})[0]\n",
    "        return generate\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.03904053484557673\n",
      "training loss: 0.028187288959413668\n",
      "training loss: 0.026341458865733795\n",
      "training loss: 0.025063653662570087\n",
      "training loss: 0.024165886772201436\n",
      "training loss: 0.02337638057780136\n",
      "training loss: 0.022733065961093493\n",
      "training loss: 0.02219254860798917\n",
      "training loss: 0.021708957697645824\n",
      "training loss: 0.02127559681453192\n",
      "training loss: 0.020911850563749626\n",
      "training loss: 0.020551289029371745\n",
      "training loss: 0.020219647489820844\n",
      "training loss: 0.019956808231951964\n",
      "training loss: 0.019682119637963565\n",
      "training loss: 0.01947070870143224\n",
      "training loss: 0.0192058188928541\n",
      "training loss: 0.019014924651846572\n",
      "training loss: 0.018804024831398727\n",
      "training loss: 0.018638461600607937\n",
      "training loss: 0.018456113992222634\n",
      "training loss: 0.01829720855882981\n",
      "training loss: 0.018150514486052087\n",
      "training loss: 0.018001362821859445\n",
      "training loss: 0.01786110171150917\n",
      "training loss: 0.017727678077872046\n",
      "training loss: 0.017591157660369148\n",
      "training loss: 0.017471624931645707\n",
      "training loss: 0.017355479147760207\n",
      "training loss: 0.017247040454022186\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "emb_en = np.random.uniform(low=-0.1, high=0.1, size=(en_total_words, hidden_size))\n",
    "emb_cn = np.random.uniform(low=-0.1, high=0.1, size=(cn_total_words, hidden_size))\n",
    "model = Seq2Seq(hidden_size, num_layers, emb_en, emb_cn)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "epoch = 0\n",
    "n_epochs = 30\n",
    "# print(sess.run(model.decoder_state))\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    total_loss = 0 \n",
    "    total_num_ins = 0\n",
    "    for (encoder_inputs, encoder_length, mb_y, mb_y_mask) in train_data:\n",
    "        decoder_inputs = mb_y[:, :-1]\n",
    "        decoder_target = mb_y[:, 1:]\n",
    "#         print(encoder_length.sum(1).shape)\n",
    "        loss = model.train(sess, encoder_inputs, encoder_length.sum(1), decoder_inputs, decoder_target, mb_y_mask[:, :-1])\n",
    "        total_loss += loss\n",
    "        total_num_ins += mb_y.shape[0]\n",
    "    print(\"training loss: {}\".format(total_loss / total_num_ins))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试一些句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOS', 'Can', 'he', 'speak', 'English?', 'EOS']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ca71f8884099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minv_cn_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_inputs = [inv_en_dict[c] for c in train_data[11][0][2]]\n",
    "print(encoder_inputs)\n",
    "encoder_inputs = [en_dict.get(e, 0) for e in encoder_inputs]\n",
    "encoder_inputs = np.asarray(encoder_inputs).reshape(1, -1)\n",
    "encoder_length = np.asarray([encoder_inputs.shape[1]]).reshape(-1)\n",
    "res = model.generate(sess, encoder_inputs, encoder_length).flatten()\n",
    "\n",
    "res = [inv_cn_dict[r] for r in res]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework\n",
    "- 我的代码全部是用的train dataset，同学们请尝试使用dev set做early stopping，存下在dev set上最好的模型。然后在test set上尝试生成一些句子，记录下一些有趣的结果。\n",
    "- 由于我的模型是一个基本的sequence to sequence模型，效果不会特别好。请同学们找几个方向尝试改进模型。以下是几个建议尝试的方向\n",
    "    - 把encoder改成bidirectional RNN\n",
    "    - 我用的是GRUCell，同学们可以尝试RNNCell和LSTMCell看看效果如何。\n",
    "    - 给decoder加上attention，可以参考tf.contrib.seq2seq\n",
    "    - 尝试beam search in generate\n",
    "    - try multi layer RNN for encoder or decoder\n",
    "    - 同学们也可自行寻找网上的论文看看有没有别的好方法\n",
    "- 我的代码不保证没有bug，所有同学们如果发现有任何的bug欢迎汇报给我，更鼓励同学们在交流群中讨论（批判）我的代码，更重要的是改进我的代码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
