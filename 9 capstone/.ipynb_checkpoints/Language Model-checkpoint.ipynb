{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LSTM to Generate Next Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Training dataset contains 750 sentences\n",
      "In total, 8809 words.\n",
      "Different words : 1048\n",
      "---------------------------------------------------------\n",
      "First 10 words in training dataset are :\n",
      " <s> Sue stuck with dance and loved it . </s>\n"
     ]
    }
   ],
   "source": [
    "train_data = './bobsue-data/bobsue.lm.test.txt'\n",
    "\n",
    "# Read the dataset as lines\n",
    "with codecs.open(train_data, 'r', 'utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "# See how many lines we have in train dataset \n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Training dataset contains {} sentences\".format(len(lines)))\n",
    "\n",
    "# Convert the lines into words\n",
    "sents = [line.split() for line in lines]\n",
    "words = [word for sent in sents for word in sent]\n",
    "\n",
    "# See how many words in train dataset\n",
    "print(\"In total, {} words.\".format(len(words)))\n",
    "print('Different words :', len(set(words)))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"First 10 words in training dataset are :\\n\", ' '.join(words[:10]))\n",
    "\n",
    "# Set free memory\n",
    "del lines, sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole vocabulary contains 1498 words.\n"
     ]
    }
   ],
   "source": [
    "vocab_file = './bobsue-data/bobsue.voc.txt'\n",
    "\n",
    "# Read vocabulary file\n",
    "with codecs.open(vocab_file, 'r', 'utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse lines -> vocabulary \n",
    "vocabulary = [line.split()[0] for line in lines if line != '\\n']\n",
    "\n",
    "# Print out information about the vocabulary\n",
    "print(\"Whole vocabulary contains {} words.\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create lookup table\n",
    "def create_lookup_table(vocab, words):\n",
    "    \"\"\" Create lookup table from vocabulary and words\n",
    "   \n",
    "    Args:\n",
    "        vocab: list(str): Vocabulary\n",
    "        words: list(str): List of words that needs to be transformed into index\n",
    "    \n",
    "    Returns:\n",
    "        index_to_word: dict{ int : str }: index -> word\n",
    "        word_to_index: dict{ str : int }: word -> index\n",
    "        word_index: list(int): words -> index of words according to word_to_index\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build index -> word and word -> index\n",
    "    index_to_word = {key: word for key, word in enumerate(vocab)}\n",
    "    word_to_index = {word: key for key, word in enumerate(vocab)} \n",
    "    \n",
    "    # Parse words list -> word index \n",
    "    word_index = [word_to_index[word] for word in words]\n",
    "    \n",
    "    return index_to_word, word_to_index, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parsing, the first 10 words' index are:\n",
      "[0, 7, 1036, 31, 392, 10, 70, 20, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "index_to_word, word_to_index, word_index = create_lookup_table(vocabulary, words)\n",
    "print(\"After parsing, the first 10 words' index are:\")\n",
    "print(word_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get number of batch data\n",
    "def get_batch(word_index, num_batches, seq_length):\n",
    "    \"\"\" Randomly get several batches of data from whole dataset\n",
    "    \n",
    "    Args:\n",
    "        word_index : list(int): List of index of words\n",
    "        num_batches: int: Number of batches\n",
    "        seq_length : int: sequence length\n",
    "        \n",
    "    Returns:\n",
    "        x_batches  : list(list(int)) :  shape = (num_batches, seq_length)\n",
    "        y_batches  : list(list(int)) :  shape = (num_batches, seq_length)\n",
    "    \"\"\"\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    max_start_index = len(word_index) - seq_length - 1\n",
    "    for _ in range(num_batches):\n",
    "        start = random.randint(0, max_start_index )\n",
    "        x_input  = word_index[ start   : start+seq_length   ]\n",
    "        y_output = word_index[ start+1 : start+seq_length+1 ]\n",
    "        \n",
    "        x_batches.append(x_input)\n",
    "        y_batches.append(y_output)\n",
    "    \n",
    "    return np.array(x_batches), np.array(y_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batches, y_batches = get_batch(word_index, 30, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
