{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordVector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordEmbedding(object):\n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        '''\n",
    "        @fname : String. File path to the zipped wiki word embbeding\n",
    "        '''\n",
    "        with zipfile.ZipFile(fname) as z:\n",
    "            filename = z.namelist()[0]\n",
    "            with z.open(filename) as f:\n",
    "                line = f.readline()\n",
    "                self._dict_size, self._embed_dim = [int(s) for s in line.split()]\n",
    "                self._embedding = np.zeros((self._dict_size, self._embed_dim), dtype=np.float32)\n",
    "                self._word2index = dict()\n",
    "                self._index2word = dict()\n",
    "                for i in range(self._dict_size):\n",
    "                    line = f.readline().split()\n",
    "                    word = line[0].decode('utf-8', 'ignore')\n",
    "                    self._word2index[word] = i\n",
    "                    self._index2word[i] = word\n",
    "                    self._embedding[i] = np.array([float(x) for x in line[1:]])\n",
    "       \n",
    "    # Getters\n",
    "    \n",
    "    def dict_size(self):\n",
    "        return self._dict_size\n",
    "    \n",
    "    def embed_dim(self):\n",
    "        return self._embed_dim\n",
    "    \n",
    "    def words(self):\n",
    "        return self._word2index.keys()\n",
    "    \n",
    "    @property\n",
    "    def embedding(self):\n",
    "        return self._embedding\n",
    "    \n",
    "    def word2index(self, word):\n",
    "        \"\"\"\n",
    "        @word: String. Return word if word exists in index, else return index of 'unknown'\n",
    "        \"\"\" \n",
    "        \n",
    "        if word in self._word2index:\n",
    "            return self._word2index[word]\n",
    "        else:\n",
    "            return self._word2index['unknown']\n",
    "        \n",
    "    def index2word(self, index):\n",
    "        \"\"\"\n",
    "        @index: int. Return word if index is in range\n",
    "        \"\"\"\n",
    "        \n",
    "        assert index > 0 and index < self._dict_size\n",
    "        return self._index2word[index]\n",
    "    \n",
    "    def wordvec(self, word):\n",
    "        '''\n",
    "        @word: String. Return word vector of word if word exists in the dictonary \n",
    "        else return the word embedding of \"unknown\"\n",
    "        '''\n",
    "\n",
    "        idx = self.word2index(word)\n",
    "        return self._embedding[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(fname):\n",
    "        with open(fname, 'rb') as fin:\n",
    "            return cPickle.load(fin)\n",
    "    \n",
    "    @staticmethod\n",
    "    def save(fname, model):\n",
    "        with open(fname, 'wb') as fout:\n",
    "            cPickle.save(model, fout)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import csv\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_positive_filename = \"./data/rt-polarity.pos\"\n",
    "mr_negative_filename = \"./data/rt-polarity.neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_positive_list, mr_negative_list = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Positive File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(mr_positive_filename, 'r', errors='ignore') as fin:\n",
    "    for line in fin:\n",
    "        review = line.lower()\n",
    "        review = review.replace('-', ' ')\n",
    "        review = ''.join([ch for ch in review if ch.isalpha() or ch == ' '])\n",
    "        words = review.split()\n",
    "        words = filter(lambda x : len(x) > 1, words)\n",
    "        review = ' '.join(words)\n",
    "        mr_positive_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the st centurys new conan and that hes going to make splash even greater than arnold schwarzenegger jean claud van damme or steven segal',\n",
       " 'the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that column of words cannot adequately describe co writerdirector peter jacksons expanded vision of tolkiens middle earth',\n",
       " 'effective but too tepid biopic',\n",
       " 'if you sometimes like to go to the movies to have fun wasabi is good place to start',\n",
       " 'emerges as something rare an issue movie thats so honest and keenly observed that it doesnt feel like one']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the data\n",
    "mr_positive_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Negative File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(mr_negative_filename, 'r', errors='ignore') as fin:\n",
    "    for line in fin:\n",
    "        review = line.lower()\n",
    "        review = review.replace('-', ' ')\n",
    "        review = ''.join([ch for ch in review if ch.isalpha() or ch ==' '])\n",
    "        words = review.split()\n",
    "        words = filter(lambda x : len(x) > 1, words)\n",
    "        review = ' '.join(words)\n",
    "        mr_negative_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simplistic silly and tedious',\n",
       " 'its so laddish and juvenile only teenage boys could possibly find it funny',\n",
       " 'exploitative and largely devoid of the depth or sophistication that would make watching such graphic treatment of the crimes bearable',\n",
       " 'garbus discards the potential for pathological study exhuming instead the skewed melodrama of the circumstantial situation',\n",
       " 'visually flashy but narratively opaque and emotionally vapid exercise in style and mystification']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the data\n",
    "mr_negative_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_index = list(range(len(mr_positive_list)))\n",
    "neg_index = list(range(len(mr_negative_list)))\n",
    "\n",
    "random.shuffle(pos_index)\n",
    "random.shuffle(neg_index)\n",
    "\n",
    "mr_positive_list = list(map(lambda x: mr_positive_list[x], pos_index))\n",
    "mr_negative_list = list(map(lambda x: mr_negative_list[x], neg_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Shuffled Review into files\n",
    "with open('./data/mr-polarity.pos', 'w') as fout:\n",
    "    for line in mr_positive_list:\n",
    "        fout.write(line+'\\n')\n",
    "with open('./data/mr-polarity.neg', 'w') as fout:\n",
    "    for line in mr_negative_list:\n",
    "        fout.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Merge Positive and Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data from files\n",
    "mr_txt, mr_label = [], []\n",
    "with open('./data/mr-polarity.pos', 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    mr_txt.extend(lines)\n",
    "    mr_label.extend([1]*len(lines))\n",
    "with open('./data/mr-polarity.neg', 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    mr_txt.extend(lines)\n",
    "    mr_label.extend([0]*len(lines)) \n",
    "assert len(mr_txt) == len(mr_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random merge the data\n",
    "data_size = len(mr_txt)\n",
    "random_index = np.arange(data_size)\n",
    "np.random.shuffle(random_index)\n",
    "mr_txt = list(np.asarray(mr_txt)[random_index])\n",
    "mr_label = list(np.asarray(mr_label)[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Training Dataset and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create word embeddings\n",
    "word_embedding = WordEmbedding('./data/embeddings.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get blank index and word embeddings' dimension\n",
    "embed_dim = word_embedding.embed_dim()\n",
    "blank_index = word_embedding.word2index('</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-vector representation, zero-padding all the sentences to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start and end positions are '</s>'\n",
    "max_len = 52\n",
    "mr_insts = np.zeros((data_size, max_len, embed_dim), dtype=np.float32)\n",
    "mr_labels = np.asarray(mr_label)[:,np.newaxis]\n",
    "for i, sent in enumerate(mr_txt):\n",
    "    words = sent.split()\n",
    "    words = [word.lower() for word in words]\n",
    "    l = min( len(words), max_len - 2 )\n",
    "    # vectors = np.zeros((len(words)+2, embed_dim), dtype=np.float32)\n",
    "    mr_insts[i, 1: l+1, :] = np.asarray([word_embedding.wordvec(word) for word in words[:l]])\n",
    "    mr_insts[i, 0, :] = mr_insts[i, l+1, :] = word_embedding.wordvec(\"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive dataset percentage 0.5.\n",
      "Negative dataset percentage 0.5.\n"
     ]
    }
   ],
   "source": [
    "pos_count = np.sum(mr_label)\n",
    "print('Positive dataset percentage {:.3g}.'.format(pos_count/len(mr_label)))\n",
    "print('Negative dataset percentage {:.3g}.'.format(1-pos_count/len(mr_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition the data -> 0.7 / 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = 2\n",
    "num_train = int(data_size * 0.7)\n",
    "num_test = data_size - num_train\n",
    "\n",
    "train_insts, train_labels = mr_insts[:num_train, :, :], to_categorical(mr_label[:num_train], num_classes)\n",
    "test_insts, test_labels = mr_insts[num_train:, :, :], to_categorical(mr_label[num_train:], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"gru\"\n",
    "ATTENTION = True\n",
    "LEARN_RATE = 0.001\n",
    "BATCH_SIZE = 20\n",
    "INPUT_SHAPE = [max_len, embed_dim]\n",
    "EPOCHE = 10\n",
    "NUM_HIDDEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, GlobalAveragePooling1D, AveragePooling1D, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "if MODEL_TYPE == 'rnn':\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(NUM_HIDDEN, input_shape=INPUT_SHAPE, return_sequences=True))\n",
    "    model.add(GlobalAveragePooling1D())                    # Mean Pooling\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "elif MODEL_TYPE == 'gru':\n",
    "    model = Sequential()\n",
    "    model.add(GRU(NUM_HIDDEN, input_shape=INPUT_SHAPE, return_sequences=True))\n",
    "    model.add(GlobalAveragePooling1D())                    # Mean Pooling\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "elif MODEL_TYPE == 'lstm':\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(NUM_HIDDEN, input_shape=INPUT_SHAPE, return_sequences=True))\n",
    "    model.add(GlobalAveragePooling1D())                    # Mean Pooling\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "else:\n",
    "    raise NameError(\"Unsupported model type\")\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 52, 100)           45300     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 45,502\n",
      "Trainable params: 45,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46s - loss: 0.6893 - acc: 0.5592\n",
      "Epoch 2/10\n",
      "45s - loss: 0.6823 - acc: 0.5988\n",
      "Epoch 3/10\n",
      "43s - loss: 0.6828 - acc: 0.5992\n",
      "Epoch 4/10\n",
      "43s - loss: 0.6896 - acc: 0.5278\n",
      "Epoch 5/10\n",
      "45s - loss: 0.6810 - acc: 0.5769\n",
      "Epoch 6/10\n",
      "45s - loss: 0.6877 - acc: 0.5436\n",
      "Epoch 7/10\n",
      "44s - loss: 0.6915 - acc: 0.5143\n",
      "Epoch 8/10\n",
      "45s - loss: 0.6881 - acc: 0.5396\n",
      "Epoch 9/10\n",
      "44s - loss: 0.6858 - acc: 0.5543\n",
      "Epoch 10/10\n",
      "43s - loss: 0.6867 - acc: 0.5446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b22e668>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_insts, train_labels, epochs=EPOCHE, batch_size=BATCH_SIZE, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
