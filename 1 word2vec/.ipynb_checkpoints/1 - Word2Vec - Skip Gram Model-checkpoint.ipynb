{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Unary Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_URL   = \"http://mattmahoney.net/dc/text8.zip\"      # 下载文件的URL\n",
    "DATA_FOLDER    = \"./data/\"                                  # 存放数据文件的文件夹路径\n",
    "FILE_NAME      = \"text8.zip\"                                # 数据文件的名称\n",
    "EXPECTED_BYTES = 31344016                                   # 文件的 bytes 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个创建本地文件夹的函数\n",
    "# \n",
    "# 参数\n",
    "# path              : 创建路径\n",
    "#\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "# 获得当前文件的 bytes 大小\n",
    "# \n",
    "# 参数\n",
    "# file_path         : 文件的路径\n",
    "#\n",
    "# return            : 当前文件的大小\n",
    "# \n",
    "def get_bytes(file_path):\n",
    "    \n",
    "    # 获得文件的描述性数据\n",
    "    file_stats = os.stat(file_path)\n",
    "    \n",
    "    # 返回文件的大小\n",
    "    return file_stats.st_size\n",
    "    \n",
    "# 检查数据的大小是否正确，用来检查是否下载了 “ 完整 ” 的数据集\n",
    "# 如果文件大小不符合所期待的大小，则抛出异常\n",
    "# \n",
    "# 参数\n",
    "# file_path         : 文件路径\n",
    "# expected_bytes    : 所期待的文件的大小\n",
    "#\n",
    "def check_bytes(file_path, expected_bytes):\n",
    "    \n",
    "    # 如果不符合期待的文件大小，则抛出异常\n",
    "    assert get_bytes(file_path) == expected_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个下载数据的函数，并检查下载的数据是否完整的被下载了\n",
    "#\n",
    "# 参数\n",
    "# source_url        : 文件 URL 下载路径\n",
    "# download_folder   : 下载到本地 文件夹 的名字\n",
    "# file_name         : 文件名\n",
    "# expected_bytes    : 文件大小\n",
    "#\n",
    "# return            : 文件的路径\n",
    "#\n",
    "def download(download_url   = DOWNLOAD_URL, \n",
    "             data_folder    = DATA_FOLDER, \n",
    "             file_name      = FILE_NAME, \n",
    "             expected_bytes = EXPECTED_BYTES):\n",
    "    \n",
    "    # 如果下载数据的路径不存在的时候，则创建一个\n",
    "    if not os.path.exists(data_folder):\n",
    "        make_dir(data_folder)\n",
    "        \n",
    "    # 下载的数据的路径为文件夹的路径 + 文件名\n",
    "    file_path = data_folder + file_name\n",
    "    \n",
    "    # 如果文件已经存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 检查文件是否完整\n",
    "        if get_bytes(file_path) == expected_bytes:\n",
    "            # 如果完整，则返回该文件的路径，不做接下来的处理了\n",
    "            print(\"Dataset already downloaded.\")\n",
    "            return file_path\n",
    "        else:\n",
    "            # 如果文件不完整，则删除文件，接下来重新下载一次\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    # 从网页上下载数据，下载文件可能会需要一段时间，请耐心等待\n",
    "    print(\"Start downloading the data, the process may take several minutes, please be patient...\")\n",
    "    file_name, _ = urllib.request.urlretrieve( url = download_url, filename = file_path )\n",
    "    \n",
    "    # 检查下载的数据是否完整\n",
    "    check_bytes( file_path, expected_bytes )\n",
    "    \n",
    "    # 返回文件的路径\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n",
      "File downloaded at path ./data/text8.zip\n"
     ]
    }
   ],
   "source": [
    "# 下载数据\n",
    "file_path = download()\n",
    "\n",
    "# 检查数据的完整性\n",
    "check_bytes( file_path, EXPECTED_BYTES )\n",
    "\n",
    "# 下载成功\n",
    "print(\"File downloaded at path\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从zip中读取所有的单词\n",
    "# 参数\n",
    "# file_path         : ZIP 文件的路径\n",
    "#\n",
    "# return            : 该文件所包含的所有单词\n",
    "#\n",
    "def read_data(file_path):\n",
    "    with zipfile.ZipFile( file = file_path ) as f:\n",
    "        # namelist : 返回在压缩目录下的所有文件\n",
    "        # read     : 读出文件的 bytes\n",
    "        words = tf.compat.as_str( f.read( f.namelist()[0] ) ).split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole content contains 17005207 words.\n",
      "The first 5 words are : ['anarchism', 'originated', 'as', 'a', 'term'].\n"
     ]
    }
   ],
   "source": [
    "# 读取单词\n",
    "words = read_data(file_path)\n",
    "\n",
    "# 打印读取的单词的长度\n",
    "print( \"The whole content contains {} words.\".format( len(words) ) )\n",
    "\n",
    "# 打印最开始的5个单词\n",
    "print( \"The first 5 words are : {}.\".format( words[:5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集：将所有单词转换为index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000    # 定义词库的大小为 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建一个 word -> index 的 dictionary，\n",
    "# 以及一个 index -> word 的 reverse_dictionary\n",
    "# 参数\n",
    "# words             : 单词输入，用来创建dictionary\n",
    "# vocab_size        : 词库的大小\n",
    "# \n",
    "# return \n",
    "#   word_index          :  输入的单词序列 转换成的 index 的序列\n",
    "#                            [ 36, 1, 0, 998, ... , 12 ]\n",
    "#   count               :  一个长度为 词库大小 的数组，每个数组的元素为 [word, count]\n",
    "#                            [ ['UNK', 3456789], ['the', 12345], ['a',12344], ... , ]\n",
    "#   dictionary          :  word -> index 的 dictionary\n",
    "#   reverse_dictionary  :  index -> word 的 reverse_dictionary\n",
    "#   \n",
    "def build_dataset(words, vocab_size):\n",
    "    \n",
    "    dictionary = {}              # 初始化空词库\n",
    "    count = [['UNK',-1]]         # 初始化 Unknown 的单词计数为-1\n",
    "    \n",
    "    # 找出出现最频繁的一组单词，加入到count数组中\n",
    "    # 单词的个数为 词库的大小 减一，因为有一个位置已经被 unknown 占据了\n",
    "    count.extend(Counter(words).most_common(vocab_size-1))\n",
    "    \n",
    "    index = 0                    # 用来记录每个单词在词库中的 index\n",
    "    \n",
    "    # 创建一个目录来存放前 1000 个单词的\n",
    "    make_dir(\"processed\")\n",
    "    with open(\"processed/vocab_1000.tsv\", \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            # 遍历 count 来生成 word -> index 的 dictionary\n",
    "            dictionary[word] = index\n",
    "            # 将前 1000 个单词写入文件中\n",
    "            if index < 1000:\n",
    "                f.write(word + '\\n')\n",
    "            index += 1\n",
    "    \n",
    "    # 将所有的从 zip 文件中读取的单词，转换成相对应的 index\n",
    "    # 如果单词存在于词库中，则返回它的index值\n",
    "    # 否则，返回 0 -- UNK\n",
    "    # 最终这个 len(word_index) == len(words)\n",
    "    word_index = [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "    \n",
    "    # word_index 中 0 出现的次数，即是 UNK 单词出现的次数\n",
    "    count[0][1] = word_index.count(0)\n",
    "    \n",
    "    # zip  : zip 函数会对传入的 iterables 进行遍历和组合，\n",
    "    #         最终返回 合并到一起 的 tuple 的数组\n",
    "    #         # zip('ABCD', 'xy') --> Ax By\n",
    "    #\n",
    "    # dict : dict 函数可以从 tuple 的数组中生成一个新的dictionary\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    # 返回 单词的对应的 index 序列，每个单词对应出现的次数表，单词 - index 表， index - 单词 表\n",
    "    return word_index, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words are : [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)].\n",
      "\n",
      "10 first index and word are : 5234/anarchism, 3081/originated, 12/as, 6/a, 195/term, 2/of, 3134/abuse, 46/first, 59/used, 156/against.\n"
     ]
    }
   ],
   "source": [
    "# 计算 word_index, count, dictionary, reverse_dictionary\n",
    "word_index, count, dictionary, reverse_dictionary = build_dataset(words, VOCAB_SIZE)\n",
    "\n",
    "# 可以节省内存\n",
    "del words\n",
    "\n",
    "# 查看一下嘴常见的 5 个单词\n",
    "print(\"5 most common words are : {}.\\n\".format( count[:5] ) )\n",
    "\n",
    "# 看一下 word_index 中的前十个 index 的值 和 对应的单词\n",
    "print(\"10 first index and word are : {}.\".format( ', '.join(\n",
    "    str(index) + '/' +(reverse_dictionary[index]) for index in word_index[:10] ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练的 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在单词的 index 序列中，随机生成一些用来训练的 batch\n",
    "# 参数\n",
    "# word_index        : 单词序列的的 index 表示，从中进行取样来生成 batch\n",
    "# batch_size        : 训练 batch 的大小\n",
    "# num_skip          : Skip Gram 模型中，从整个窗口中选取多少个不同的词作为output word\n",
    "# window_size       : 取单词来预测的 window 大小\n",
    "#\n",
    "# return            :\n",
    "#   batch           : 一个 batch size 的 input 数据\n",
    "#   labels          : batch 中每个元素预测的单词\n",
    "#\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(word_index, \n",
    "                   batch_size   = 8, \n",
    "                   num_skip     = 2, \n",
    "                   skip_window  = 1):\n",
    "    \n",
    "    global data_index                             # data_index 为全局变量，每一次取样都会对它的值进行更新\n",
    "    \n",
    "    assert batch_size % num_skip == 0             # 要求从中取出的单词数为 batch 的整除数\n",
    "    assert num_skip <= 2 * skip_window            # 取出来的单词数要小于 2倍 window 的大小\n",
    "    \n",
    "    # shape 接收 int 的 tuple，生成多维的数组\n",
    "    batch  = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1                   # 左右两侧的大小为skip window 并 加上中间词 1\n",
    "    buffer = collections.deque(maxlen=span)      # 创建一个大小为 span 的 buffer\n",
    "    \n",
    "    # 如果此时 buffer 取样越界了，则重头开始取样\n",
    "    if data_index + span >= len(word_index):\n",
    "        data_index = ( data_index + span ) % len(word_index)\n",
    "        \n",
    "    # 从 word_index 中读取一个长度为 span 的词序列放入 buffer 中，用来生成 batch\n",
    "    buffer.extend(word_index[data_index:data_index+span])\n",
    "    \n",
    "    for i in range(batch_size // num_skip):        # batch_size/num_skip 是为了取到batch size的个数，我们所需中心词的个数\n",
    "        \n",
    "        # 左右两边的词为 context words\n",
    "        context_words = [w for w in range(span) if span != skip_window]\n",
    "        \n",
    "        # 重洗一下这个 index 序列的顺序，用来随机选取 num_skip 个单词进行训练\n",
    "        random.shuffle(context_words)\n",
    "        words_to_use = collections.deque(context_words)\n",
    "        for j in range(num_skip):\n",
    "            context_word = words_to_use.pop()\n",
    "            batch[i*num_skip+j]  = buffer[skip_window]\n",
    "            labels[i*num_skip+j,0] = buffer[context_word]\n",
    "        \n",
    "        if data_index == len(word_index):       # 如果data_index已经到这个文档的末尾了，我们就从头开始\n",
    "            buffer[:] = word_index[:span]\n",
    "            data_index = span\n",
    "        else:                                   # 把整个window往后面移一个位置\n",
    "            buffer.append(word_index[data_index])\n",
    "            data_index += 1\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3081 originated -> [12] as\n",
      "1 3081 originated -> [3081] originated\n",
      "2 12 as -> [5234] anarchism\n",
      "3 12 as -> [12] as\n",
      "4 5234 anarchism -> [3081] originated\n",
      "5 5234 anarchism -> [12] as\n",
      "6 3081 originated -> [5234] anarchism\n",
      "7 3081 originated -> [12] as\n"
     ]
    }
   ],
   "source": [
    "# 测试生成一下 batch 和 labels\n",
    "batch, labels = generate_batch(word_index)\n",
    "\n",
    "# 打印出来 input -> output\n",
    "for i in range(8):\n",
    "    print(i, batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i], reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBBED_SIZE = 64\n",
    "NUM_SAMPLED = 15\n",
    "LEARNING_RATE = 1.0\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    \"\"\"\n",
    "        Placeholders\n",
    "    \"\"\"\n",
    "    \n",
    "    # 训练时的输入 placeholder ：输入为中心词的 index 的 batch\n",
    "    target_words  = tf.placeholder(name='target_words',  shape=[BATCH_SIZE],   dtype=tf.int32)\n",
    "    # 训练时的输出 placeholder ：输出为 context word 的 index 的 batch\n",
    "    context_words = tf.placeholder(name='context_words', shape=[BATCH_SIZE,1], dtype=tf.int32)\n",
    "    \n",
    "    \"\"\"\n",
    "        Variables\n",
    "    \"\"\"\n",
    "    \n",
    "    # 训练时不断的更新这个 embbding lookup table，得到最终的每个单词的词向量\n",
    "    embbedings = tf.Variable(                      # 输入的 Lookup Table\n",
    "        tf.random_uniform(                         # 随机生成的数据\n",
    "            [VOCAB_SIZE,EMBBED_SIZE],              # tensor 的大小为 VOCAB_SIZE * EMBBED_SIZE\n",
    "            -1.0, 1.0))                            # 范围在 -1.0 到 1.0 之间\n",
    "    # 通过在 lookup table 中匹配中心词，找到的 target words 的词向量\n",
    "    embed = tf.nn.embedding_lookup( embbedings, target_words )\n",
    "    # 在训练中会用来更新的权值矩阵 W\n",
    "    nce_weights = tf.Variable( \n",
    "        tf.truncated_normal(\n",
    "            shape=[VOCAB_SIZE, EMBBED_SIZE], \n",
    "            stddev=1.0/(EMBBED_SIZE ** 0.5)) )\n",
    "    # 在训练中用来更新的偏差矩阵 b\n",
    "    nce_bais = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "    \n",
    "    \"\"\"\n",
    "        Loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    # 用 negative sampling 方法来简化计算\n",
    "    loss = tf.reduce_mean( \n",
    "        tf.nn.nce_loss(\n",
    "            inputs=embed,                         # 输入为由 target words 在当前 lookup table 中找到的词向量\n",
    "            labels=context_words,                 # 期待由 target words 能预测出来的 context words\n",
    "            weights=nce_weights,                  # 权值矩阵\n",
    "            biases=nce_bais,                      # 偏差矩阵\n",
    "            num_classes=VOCAB_SIZE,               # 最终分类会得到的类别总数为 所有单词的个数\n",
    "            num_sampled=NUM_SAMPLED) )            # 在 negative sampling 中的取样个数\n",
    "    \n",
    "    \"\"\"\n",
    "        Training step\n",
    "    \"\"\"\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE    = 8       # 训练 batch 的大小\n",
    "NUM_SKIP      = 2       # Skip Gram 模型中，从整个窗口中选取多少个不同的词作为output word\n",
    "SKIP_WINDOW   = 1       # 取单词来预测的 window 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
