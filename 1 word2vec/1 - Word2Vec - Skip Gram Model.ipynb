{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_URL = \"http://mattmahoney.net/dc/text8.zip\"      # 下载文件的URL\n",
    "DATA_FOLDER = \"./data/\"                                   # 存放数据文件的文件夹路径\n",
    "FILE_NAME = \"text8.zip\"                                   # 数据文件的名称\n",
    "EXPECTED_BYTES = 31344016                                 # 文件的 bytes 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个创建本地文件夹的函数\n",
    "# \n",
    "# 参数\n",
    "# path              : 创建路径\n",
    "#\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "# 获得当前文件的 bytes 大小\n",
    "# \n",
    "# 参数\n",
    "# file_path         : 文件的路径\n",
    "#\n",
    "# return            : 当前文件的大小\n",
    "# \n",
    "def get_bytes(file_path):\n",
    "    \n",
    "    # 获得文件的描述性数据\n",
    "    file_stats = os.stat(file_path)\n",
    "    \n",
    "    # 返回文件的大小\n",
    "    return file_stats.st_size\n",
    "    \n",
    "# 检查数据的大小是否正确，用来检查是否下载了 “ 完整 ” 的数据集\n",
    "# 如果文件大小不符合所期待的大小，则抛出异常\n",
    "# \n",
    "# 参数\n",
    "# file_path         : 文件路径\n",
    "# expected_bytes    : 所期待的文件的大小\n",
    "#\n",
    "def check_bytes(file_path, expected_bytes):\n",
    "    \n",
    "    # 如果不符合期待的文件大小，则抛出异常\n",
    "    if get_bytes(file_path) != expected_bytes:\n",
    "        raise Exception(\"File has not been download successfully, please try download again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个下载数据的函数，并检查下载的数据是否完整的被下载了\n",
    "#\n",
    "# 参数\n",
    "# source_url        : 文件 URL 下载路径\n",
    "# download_folder   : 下载到本地 文件夹 的名字\n",
    "# file_name         : 文件名\n",
    "# expected_bytes    : 文件大小\n",
    "#\n",
    "# return            : 文件的路径\n",
    "#\n",
    "def download(download_url   = DOWNLOAD_URL, \n",
    "             data_folder    = DATA_FOLDER, \n",
    "             file_name      = FILE_NAME, \n",
    "             expected_bytes = EXPECTED_BYTES):\n",
    "    \n",
    "    # 如果下载数据的路径不存在的时候，则创建一个\n",
    "    if not os.path.exists(data_folder):\n",
    "        make_dir(data_folder)\n",
    "        \n",
    "    # 下载的数据的路径为文件夹的路径 + 文件名\n",
    "    file_path = data_folder + file_name\n",
    "    \n",
    "    # 如果文件已经存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 检查文件是否完整\n",
    "        if get_bytes(file_path) == expected_bytes:\n",
    "            # 如果完整，则返回该文件的路径，不做接下来的处理了\n",
    "            print(\"Dataset already downloaded.\")\n",
    "            return file_path\n",
    "        else:\n",
    "            # 如果文件不完整，则删除文件，接下来重新下载一次\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    # 从网页上下载数据，下载文件可能会需要一段时间，请耐心等待\n",
    "    print(\"Start downloading the data, the process may take several minutes, please be patient...\")\n",
    "    file_name, _ = urllib.request.urlretrieve( url = download_url, filename = file_path )\n",
    "    \n",
    "    # 检查下载的数据是否完整\n",
    "    check_bytes( file_path, expected_bytes )\n",
    "    \n",
    "    # 返回文件的路径\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n",
      "File downloaded at path ./data/text8.zip\n"
     ]
    }
   ],
   "source": [
    "# 下载数据\n",
    "file_path = download()\n",
    "\n",
    "# 检查数据的完整性\n",
    "check_bytes( file_path, EXPECTED_BYTES )\n",
    "\n",
    "# 下载成功\n",
    "print(\"File downloaded at path\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从zip中读取所有的单词\n",
    "# 参数\n",
    "# file_path         : ZIP 文件的路径\n",
    "#\n",
    "# return            : 该文件所包含的所有单词\n",
    "#\n",
    "def read_data(file_path):\n",
    "    with zipfile.ZipFile( file = file_path ) as f:\n",
    "        # namelist : 返回在压缩目录下的所有文件\n",
    "        # read     : 读出文件的 bytes\n",
    "        words = tf.compat.as_str( f.read( f.namelist()[0] ) ).split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole content contains 17005207 words.\n",
      "The first 5 words are : ['anarchism', 'originated', 'as', 'a', 'term'].\n"
     ]
    }
   ],
   "source": [
    "# 读取单词\n",
    "words = read_data(file_path)\n",
    "\n",
    "# 打印读取的单词的长度\n",
    "print( \"The whole content contains {} words.\".format( len(words) ) )\n",
    "\n",
    "# 打印最开始的5个单词\n",
    "print( \"The first 5 words are : {}.\".format( words[:5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集：将所有单词转换为index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义词库的大小为 50000\n",
    "VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建一个 word -> index 的 dictionary，\n",
    "# 以及一个 index -> word 的 reverse_dictionary\n",
    "# 参数\n",
    "# words             : 单词输入，用来创建dictionary\n",
    "# vocab_size        : 词库的大小\n",
    "# \n",
    "# return \n",
    "#   word_index          :  输入的单词序列 转换成的 index 的序列\n",
    "#                            [ 36, 1, 0, 998, ... , 12 ]\n",
    "#   count               :  一个长度为 词库大小 的数组，每个数组的元素为 [word, count]\n",
    "#                            [ ['UNK', 3456789], ['the', 12345], ['a',12344], ... , ]\n",
    "#   dictionary          :  word -> index 的 dictionary\n",
    "#   reverse_dictionary  :  index -> word 的 reverse_dictionary\n",
    "#   \n",
    "def build_dataset(words, vocab_size):\n",
    "    \n",
    "    dictionary = {}              # 初始化空词库\n",
    "    count = [['UNK',-1]]         # 初始化 Unknown 的单词计数为-1\n",
    "    \n",
    "    # 找出最出现的单词，加入到count数组中\n",
    "    # 单词的个数为 词库的大小 减一，因为有一个位置已经被 unknown 占据了\n",
    "    count.extend(Counter(words).most_common(vocab_size-1))\n",
    "    \n",
    "    index = 0                    # 用来记录每个单词在词库中的 index\n",
    "    \n",
    "    # 创建一个目录来存放前 1000 个单词的\n",
    "    make_dir(\"processed\")\n",
    "    with open(\"processed/vocab_1000.tsv\", \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            # 遍历 count 来生成 word -> index 的 dictionary\n",
    "            dictionary[word] = index\n",
    "            # 将前 1000 个单词写入文件中\n",
    "            if index < 1000:\n",
    "                f.write(word + '\\n')\n",
    "            index += 1\n",
    "    \n",
    "    # 将所有的从 zip 文件中读取的单词，转换成相对应的 index\n",
    "    # 如果单词存在于词库中，则返回它的index值\n",
    "    # 否则，返回 0 -- UNK\n",
    "    # 最终这个 len(word_index) == len(words)\n",
    "    word_index = [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "    \n",
    "    # word_index 中 0 出现的次数，即是 UNK 单词出现的次数\n",
    "    count[0][1] = word_index.count(0)\n",
    "    \n",
    "    # zip  : zip 函数会对传入的 iterables 进行遍历和组合，\n",
    "    #         最终返回 合并到一起 的 tuple 的数组\n",
    "    #         # zip('ABCD', 'xy') --> Ax By\n",
    "    #\n",
    "    # dict : dict 函数可以从 tuple 的数组中生成一个新的dictionary\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    # 返回 单词的对应的 index 序列，每个单词对应出现的次数表，单词 - index 表， index - 单词 表\n",
    "    return word_index, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words are : [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)].\n",
      "\n",
      "10 first word and index are : 5234/anarchism, 3081/originated, 12/as, 6/a, 195/term, 2/of, 3134/abuse, 46/first, 59/used, 156/against.\n"
     ]
    }
   ],
   "source": [
    "# 计算 word_index, count, dictionary, reverse_dictionary\n",
    "word_index, count, dictionary, reverse_dictionary = build_dataset(words, VOCAB_SIZE)\n",
    "\n",
    "# 可以节省内存\n",
    "del words\n",
    "\n",
    "# 查看一下嘴常见的 5 个单词\n",
    "print(\"5 most common words are : {}.\\n\".format( count[:5] ) )\n",
    "\n",
    "# 看一下 word_index 中的前十个 index 的值 和 对应的单词\n",
    "print(\"10 first index and word are : {}.\".format( ', '.join(\n",
    "    str(index) + '/' +(reverse_dictionary[index]) for index in word_index[:10] ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练的 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
