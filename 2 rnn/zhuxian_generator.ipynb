{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_PATH = './data/诛仙.txt'\n",
    "# Whether or not use Chinese split words, if false, use single chars to feed\n",
    "USE_SPLIT = True                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the book as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3126269 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "\n",
    "with codecs.open(FILE_PATH, 'r', 'utf-8') as book_file:\n",
    "    corpus_raw += book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Corpus\n",
    "##### Create lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text, use_split=USE_SPLIT):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocab\n",
    "    :param text: The corpus text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words = list(jieba.cut(text))\n",
    "    vocab = set(words) if use_split else set(text)\n",
    "    \n",
    "    int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "    vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    \n",
    "    if use_split:\n",
    "        text_index = [vocab_to_int[word] for word in words]\n",
    "    else:\n",
    "        text_index = [vocab_to_int[word] for word in text]\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, text_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 2050766, number of Chinese words in text : 38012\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, corpus_int = create_lookup_tables(corpus_raw)\n",
    "print(\"Vocabulary size : {}, number of Chinese words in text : {}\".format(len(corpus_int), len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network\n",
    "### Batch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target data\n",
    "    :param int_text: text with words replaced by their ids\n",
    "    :param batch_size: the size that each batch of data should be\n",
    "    :param seq_length: the length of each sequence\n",
    "    :return: batches of data as a numpy array\n",
    "    \"\"\"\n",
    "    words_per_batch = batch_size * seq_length\n",
    "    num_batches = len(int_text)//words_per_batch\n",
    "    int_text = int_text[:num_batches*words_per_batch]\n",
    "    y = np.array(int_text[1:] + [int_text[0]])\n",
    "    x = np.array(int_text)\n",
    "    \n",
    "    x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    batch_data = list(zip(x_batches, y_batches))\n",
    "    \n",
    "    return np.array(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 512\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "keep_prob = 0.7\n",
    "embed_dim = 512\n",
    "seq_length = 30\n",
    "learning_rate = 0.001\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():    \n",
    "    \n",
    "    # Initialize input placeholders\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    # Calculate text attributes\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Build the RNN cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "    drop_cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop_cell] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    # Create word embedding as input to RNN\n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "    # Take RNN output and make logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Calculate the probability of generating each word\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    # Define loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_text_shape[0], input_text_shape[1]])\n",
    "    )\n",
    "    \n",
    "    # Learning rate optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Batches : 133\n",
      "Epoch   1 Batch    1/133   train_loss = 10.546\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-b9bd49c31fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             }\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(corpus_int, batch_size, seq_length)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Num Batches :\", num_batches)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "            \n",
    "            if batch_index % 5 == 0:\n",
    "                time_elapsed = time.time() - start_time\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}   time_elapsed = {:.3f}   time_remaining = {:.0f}'.format(\n",
    "                    epoch + 1,\n",
    "                    batch_index + 1,\n",
    "                    len(batches),\n",
    "                    train_loss,\n",
    "                    time_elapsed,\n",
    "                    ((num_batches * num_epochs)/((epoch + 1) * (batch_index + 1))) * time_elapsed - time_elapsed))\n",
    "                \n",
    "                # save model every 5 batches\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, save_dir)\n",
    "                print('Model Trained and Saved')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "### Pick a Random Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Graph and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "我 海上 日常用品 温润 抛开 以手 云海 神众 爬起来 异常 振荡 之至 激动不已 英雄末路 木架 间手 连个 手下败将 清光 以目 向後看 与此同时 笑了起来 佳品 越斗 恍若 嘘声四起 仰观 带来 和睦 逐级 小灰似 敬若天神 清泉 越慢 退居 称谢 上红下 栽 端端正正 二物 黄泉 浓如墨 连林 十道 猜测 小灰挠 龙头 千次 修复 四指 不是故意 哐啷 飞奔而去 大洞 诸多 离闻 微一 光压 神宫 李洵 拗断 笑了笑 二百四十 任何人 放不下 白白净净 暗叹 互相残杀 檐角 山中 宾主 废话 似林 乎 山路 搭理 过时 塔 纵然 玩物 本事 粗鲁 空牌 所惊 第二排 侍立 镇族 决定 人海茫茫 千百年 公然 路滑 狂噬 闪出 不整 傻小子 貌美如花 光剑 地气 木板门 这比 冷着 放倒 莫测高深 低眉 熊掌 冷夜 头绪 小灰喝 后患 临别时 暗地 绷 灾噩 连累 一提 高过 手里 直非 竹林精舍 曾带 帮衬 弄 相别 咬紧 但事 怪力 举杯 回谷 人挥 甩进 有意无意 能以 煌煌 密传 一魂 撕裂 色彩 这么久 惊起 残光 巨啸 看书 夯量 火苗 出卖 平锋玉尺 能胜到 满场 如牛 香袋 所赐 满满当当 这具 何愁 逆流 鼓七鸣 告慰 直攻 预料中 独居 地贴 钟乳石 屡屡 爪子 礼道 鬼眸 一真 不得已而为之 御岩术 杀人狂 那如水 叱骂 合适 大笑不止 大事 满座 不信 另一端 浮云 蹦起 般的 大批 形若 弱女子 说不下去 暂避 地别 理 近来 悠然 留住 雪琪面 功业 神秘莫测 通入 欲用 厮斗声 远看 自治州 死亡 密语 勒紧 提心吊胆 电击 比较 全败 前面 冲前 得心应手 研究 既入 已亮 其实 心不死 不去 痛恨 沁入 紧绷绷 零乱 吼叫声 咒来 精至 共分 极其重要 铃中 恶战 大家 还令 互通 娇媚 倒行逆施 命是 低眉 通天彻地 我怪 若无其事 迸发出 自不必说 持开 拖来拖去 征站 抬头 事关重大 推著 自许 两次 敌得过 精神支柱 轰然 扶摇 不让 四分之三 帮助 生之力 面前 青土 真火 错路 学田 打赌 冠绝 旧识 心障 并不比 修到 重插 式微 少说 虚怀若谷 那涸 无耻之徒 吸食 雅观 不肯 主毒 或用 八人 欲言又止 人类 几本 大势所趋 而颂 不绝 侄女 六峰 按时 弦 停停 之能 迈去 红着脸 见天 小环轻 好几眼 握著 数目 加紧 莲花 胸脯 方可 猝不及防 手顿 柔美 驱起 微露 而进 僧道 流向 一宿 躲过 更厚 放上 好像 化为泡影 后跃开 蜜语 小灰似 自债 罪大恶极 扎根 先兆 源 冲进 注重 岛上 行侠 轻摆 发出声响 无怨 神众 峙 辞世 策谋 一合 各处 喷而出 天狐一 土地庙 穿衣 领陆雪琪 腹心 扫来扫去 更狂 舞到 人若望 时过境迁 策面 冒出来 六十岁 挥剑 水印 顽固 之中 如摹 鲜红 抱憾 千思万念 痉挛 容一臂 波山 渗 僵在 如奉 苦海 过五人 咧嘴一笑 深一脚浅一脚 西来 碍事 面置 点滴 更多人 水果 以站 两难 每深 摆弄 所剩 篆体 刮得 孤静 呐呐道 杨花 末端 来田 特派 射出 长蛇 刀片 绞成 镇住 印证 整晚 美食 凶地 轻轻 神道 他身 身不离 初一 怎知 照旧 中多 非到 池镇 骨裂 灵长 千百 禅室 畅通无阻 朋友 无法形容 晨昏 劝说 两幅 邪物时 黄雀 忽 不可开交 冷竣 青石 浑话 很感兴趣 苗族 望著 石阶 房子 这寐鱼 对生 地钉 此情此景 沸腾 欺师 瘦长 如倾 这团 手道 动物 浮著 往西方 袋子 水瓶 同来 满口 震耳 阻在 幽下 风吹草动 已知 素来 龙剑 空逸 地靠 自信 护法 根基深厚 分而食 虎豹 慎重 巫山 凶灵见 按上 书桌上 留手 残身 仇 鼓足 狗屁 插口 下冲 一尊 轮番 极冷 嗖 老脸 微小 忘却 尽数 圆 巧事 高亢 六面 远行 股喘 巨派 叫喊声 谈及 店铺 递过 便围 处所 纷繁 闻讯 先往 小灰大感 书卷 红扑扑 出鞘 凹槽 气体 灭顶之灾 派对 郑重其事 太甚 一架 背腰 来客 腥臭 嘶声 遥相呼应 前见 揣摩 箭在弦上 甚好 倾听 夫妇 直上 小有所成 意见 瞭解 背风 热爱 子寿 甚严 反增 鱼体 请云 竹旁 不快 他围 微薄 费心 追下去 心神不宁 开有 消受 此劫 特命 两边 较少 海里 齐昊护 帮鬼厉 善堂 追随着 万孔 属于 功业 好不容易 千金 翻起 下风 咋舌 如新 低头 毒手 吹倒 势力 古字 倒把 亲眼看到 尸山血海 手持着 上品 户外 乐观 宁杀 一语 无依 猛禽 有用吗 围得 能行 减慢 焦肉 冒上来 临走前 平钝 一一记 教中 硬着头皮 实土 仙咒声 便命 刮来 其声 之惑 甜处 红三色 琉璃 碍著 粘着 那兽 手指头 迅低 行险 汽化 狗窝 须臾之间 从那时起 往右 一辈子 架御 曾败 坚毅 五六十年 清蒸 信号 五层 随便 更响 欢欢喜喜 水粒 重器 死状 随家师 千斤 通晓 见燕虹 恁地 记在 涎 这魔道 重权 会用 稚嫩 头晕 连忙 转念之间 宽丈 当仁不让 心处 空闲 早早 但近 困难 向谷主 一空 一朵花 扯 张少侠 地冲进 势大 细长 轻吒 却拉过 而逝 夸奖 装有 收紧 多时 意气用事 相貌 跌 伸 清涩 定夺 有俗 划出 前所未有 相聚 但河 趋之若骛 龙颈 这魔道 嘘 强移 已比 拉上 草书 三代 他守 那分 真上 对错 如此说来 三力 火星 龙剑剑刃 第四部 随家师 所驭 代掌教 仙哭 环流 锋锐 低笑 申名 诵 有升 之求 粗重 为底 一望 旋 有散 以陆 这瓶儿 创立 他油 各般 想得 尚未 斗志 八层 大喝 骚乱 逐开 精致 疾念 木门 亵渎 不计其数 暗伤 中微 中正 中萧 与己无关 说不完 时局 芒刺 这越 说得上 传遍 这株 借 就象 仓促 完好无恙 战况 悠 这儿 边 敢怒不敢言 哪棵 激动 多礼 泄 眼皮 门田 宋大仁苦 地飞出 他分 青年 突兀 云朵 妄称 搅动 板 深入 打发 前花 神鬼不测 一台 如洗 弘扬 为求 不见天日 邪说 手中 故老 那九幽 魔幻 堂到 烫手 要取 五百两 处多 更大 喧嚣声 逸事 连条 虹桥 突现 生双翅 秘洞 睡著 魔神 扔 严厉 采药 用处 幽美 傻事 稳固 大致相同 再练 已上 就此 扫过去 魂不守舍 那小周 显出 闲聊 左支右绌 对撞 四两 对小灰道 高速 心痛 缝隙 伤天害理 比刚 火苗 深灰 所知 他盘 经书 人死 尽全力 苏如师 了不得 全场 如山 这术法 心乱如麻 穿破 异 总人 修 好久 惊叹 盈然 迟缓 害人 盘算 狼藉 先呆 上边 情由 呼喊 直飞 形容 战抖 银光 一伸 奇闻 四十九岁 单论 自我安慰 大椅 主持 回味 便失 获生 直响 兴味索然 相若 低 度太急 事发 两扇 大头 柴火 势如破竹 更白 厉双眉 聊到 环环相扣 逸事 几块 关键所在 穷光蛋 右转 能为 感到恐惧 小白竟 派遣 大殿 条命 自尊 空洞洞 幽绿冥火 求愿 后继有人 不喘地 座下 激动不已 姿态 冰台 红石时 撒娇 背後去 虫子 而近 强笑 眼亲 这七脉 往岛 处处 震处 用万火 不通 思慕 还害 小女 腥臭味 到口 隆隆 狗嘴里 收化 养育 山野 禁止 而啸 胸怀 追着 神物 师兄弟 哪位 握住 气力 斗见 阿弥陀佛 开窍 畏怯 弥散 守灵 一心 果真 振 四指 甲 停止 第五步 血汗钱 鼎立 千刀万剐 师伯师 地翻 中照 人通修 悲愤 直裂 处挥 共登 一肩 古尸 两眼 言语 胸脯 驱物 这大喝 学道 广 痛如针 前面 无人知晓 已现 倒 普智 那头 节哀 视如己出 情愿 造化弄人 矮矮 狗肉 示威 玄虚 想过 侧身 真非 不经意 几乎 大城 之顶 刺探 医给 厉雷 下一场 弟弟 耀眼 鉴到 散去 被困 塑像 一时间 恳切 代劳 能成 山峦起伏 砍刀 环绕 佼佼者 狂剑剑刃 年龄 骇然 落如纷 万鬼 奇书 浪子 二十五 遇 一振 一生 惧意 震恐 差点\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "prime_words = '我'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = list(jieba.cut(prime_words)) if USE_SPLIT else prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        # Get predict word\n",
    "        word_probs = probabilities[0][dyn_seq_length-1]\n",
    "        pred_word = pick_word(word_probs, int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ''.join(gen_sentences)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('generated_text.txt', \"w\") as text_file:\n",
    "    text_file.write(chapter_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
